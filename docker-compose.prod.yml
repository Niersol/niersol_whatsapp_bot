services:
  web:
    build: 
      context: ./app
      dockerfile: Dockerfile.prod
    command: gunicorn whatsapp_gpt_bot.wsgi:application --bind 0.0.0.0:8000
    volumes:
      - static_volume:/home/app/web/staticfiles
      - /srv/mediafiles:/home/app/web/mediafiles
    expose:
      - 8000
    env_file:
      - ./.env.prod
  nginx-proxy:
    container_name: nginx-proxy
    build: nginx
    restart: always
    ports:
      - 443:443
      - 80:80
    volumes:
      - static_volume:/home/app/web/staticfiles
      - /srv/mediafiles:/home/app/web/mediafiles
      - certs:/etc/nginx/certs
      - html:/usr/share/nginx/html
      - vhost:/etc/nginx/vhost.d
      - /var/run/docker.sock:/tmp/docker.sock:ro
    depends_on:
      - web

  acme-companion:
    image: nginxproxy/acme-companion
    env_file:
      - ./.env.prod.proxy-companion
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - certs:/etc/nginx/certs
      - html:/usr/share/nginx/html
      - vhost:/etc/nginx/vhost.d
      - acme:/etc/acme.sh
    depends_on:
      - nginx-proxy

  nginx:
    build: ./nginx
    volumes:
      - static_volume:/home/app/web/staticfiles
    ports:
      - 1337:80
    depends_on:
      - web

  redis:
    image: redis:latest
    command: redis-server --appendonly yes
    # No need to expose ports externally unless you want direct host access.
    # Internally, Celery and Web can use "redis:6379".

  worker:
    build: 
      context: ./app
      dockerfile: Dockerfile.prod
    command: celery -A whatsapp_gpt_bot worker -l info
    env_file:
      - ./.env.prod
    depends_on:
      - redis

volumes:
  static_volume:
  certs:
  html:
  vhost:
  acme: